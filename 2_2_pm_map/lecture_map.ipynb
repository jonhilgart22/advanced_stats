{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://image.slidesharecdn.com/04optimalestimation-151107223247-lva1-app6892/95/computational-motor-control-optimal-estimation-in-noisy-world-jaist-summer-course-16-638.jpg?cb=1446935669\"\n",
    "wwidth=\"1000\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Define Maximum A Posteriori (MAP) estimation\n",
    "- Explain the difference between MAP and Maximum Likelihood Estimation (MLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Maximum Likelihood Estimation (MLE) Redux\n",
    "-----\n",
    "\n",
    "Use maximum likelihood of the data to estimate the underlying parameter - Î˜."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Maximum A Posteriori (MAP) Estimation\n",
    "-----\n",
    "\n",
    "Sometimes we have a __priori__ information about the process whose parameters we want to estimate. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can encode such prior information in terms of a probability distribution on the parameter to be estimated. \n",
    "\n",
    "Essentially, we treat the parameter $\\theta$ as a value of an RV. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bayes Formula for $\\theta$\n",
    "------\n",
    "\n",
    "If we assume $\\theta$ is a random variable (RV), it has a\n",
    "conditional distribution on the data:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}(\\theta|x) = \\frac{\\mathbb{P}(x|\\theta)\\mathbb{P}(\\theta)}{\\mathbb{P}(x)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\n",
    "\\mathbb{P}(\\theta|x) = \\frac{\\mathbb{P}(x|\\theta)\\mathbb{P}(\\theta)}{\\mathbb{P}(x)}\n",
    "$$\n",
    "\n",
    "$\\mathbb{P}(x|\\theta)$ is the likelihood term.   \n",
    "$ \\mathbb{P}(x)$ is __prior__ probability of the data $x$.   \n",
    "$\\mathbb{P}(\\theta)$ is the __prior__ probability of the\n",
    "parameter. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-------\n",
    "\n",
    "$\\mathbb{P}(\\theta)$ means regardless of the observed data \\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_\\_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "this is the assumed probability of the parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "Where can this prior information come from?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Literally - \"Prior\" is the Past\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://static.wixstatic.com/media/2ce843_fbefd6bd5b1f4fcfb4c534932fd259ca~mv2.jpg/v1/fill/w_168,h_180/2ce843_fbefd6bd5b1f4fcfb4c534932fd259ca~mv2.jpg\" width=\"300\"/></center>\n",
    "\n",
    "It could be Theory or Practice (empirical data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Maximum A Posteriori (MAP) estimation of $\\theta$\n",
    "------\n",
    "\n",
    "Assuming $\\mathbb{P}(\\theta)$, the next step is the maximizing of this expression over the Î¸.\n",
    "\n",
    "Whatever results from that maximization is the __maximum a-posteriori (MAP) estimator for Î¸__ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Because the maximization takes place with respect to\n",
    "$\\theta$ and not $x$, we can ignore the $\\mathbb{P}(x)$ part. \n",
    "\n",
    "__We do not care about the prior of the data.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$\\displaystyle \\mathop{\\mbox{argmax }}_{\\theta}\n",
    "P (\\theta \\vert {\\bf x})\n",
    "= \\mathop{\\mbox{argmax }}_{\\theta}\n",
    "P ({\\bf x} \\vert \\theta)\n",
    "P (\\theta).$$\n",
    "\n",
    "\n",
    "Left-hand side is the posterior.   \n",
    "Right-hand side is the product of the likelihood term and the prior term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/map_fig.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A MAP estimate is the mode of the posterior distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/peak.png\" height=\"500\"/></center>\n",
    "\n",
    "MAP â€˜pullsâ€™ the prediction towards the prior.\n",
    "\n",
    "Depending on the strength of the prior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MAP is often described as a more \"natural\" style of prediction. It takes our bias into account."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The maximum likelihood estimate (MLE)\n",
    "-----\n",
    "\n",
    "The value of the parameter that maximizes the likelihood, where the likelihood is a function of the parameter and is actually __equal to the probability of the data conditioning on that parameter__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Maximum a posteriori (MAP) estimation\n",
    "----\n",
    "\n",
    "The value of the parameter that maximizes the __entire posterior distribution__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MLE Formula\n",
    "--------\n",
    "\n",
    "<center><img src=\"images/mle.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "MAP Formula\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/map_form.png\" width=\"700\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "What is the biggest limitation for both MLE and MAP?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "They are both point estimators!\n",
    "\n",
    "Point estimators are always wrong by definition for continuous variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MAP uses Bayes' theorem hence Bayesian inference\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But \"baby\" ðŸ£ Bayesian, still point estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MAP Limitation\n",
    "---\n",
    "\n",
    "\n",
    "Since there is the Prior term, the empirical data can never completely overwhelm prior.\n",
    "\n",
    "If you pick a strong (but wrong) prior, your MAP estimates will be inaccurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Example](http://stats.stackexchange.com/questions/65212/example-of-maximum-a-posteriori-estimation)   \n",
    "[Example](https://github.com/unpingco/Python-for-Signal-Processing/blob/master/MAP_Estimation.ipynb)   \n",
    "[Intro of MaxEnt](http://cmm.cit.nih.go`av/maxent`/letsgo.html)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "When MAP and MLE estimate been the same?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "1. When the MAP __prior distribution is uniform__.\n",
    "\n",
    "2. As the n -> âˆž"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "Is MLE or MAP more likely to overfit the data? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__MLE__\n",
    "\n",
    "MLE might have variance of the parameter estimates, outcome of the parameter estimate is sensitive to random variations in data \n",
    "\n",
    "Especially a problem with a small amounts of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "MAP __regularizes__ estimates by assuming that the parameters themselves are also (in addition to the data) drawn from a random process. \n",
    "\n",
    "The prior beliefs about the parameters determine what this random process looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Source](https://www.quora.com/What-is-the-difference-between-Maximum-Likelihood-ML-and-Maximum-a-Posteri-MAP-estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "More f\\*\\*\\*ing coin fliping\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"http://funnyand.com/wp-content/uploads/2015/11/Flip-A-Coin.jpg\" width=\"450\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The likelihood function for a coin flip is:\n",
    "\n",
    "$$\n",
    "\\ell(\\theta) := \\theta^k (1-\\theta)^{ (n-k) }\n",
    "$$\n",
    "\n",
    "where the probability of the coin coming up heads is $\\theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Prior probability, $\\mathbb{P}(\\theta)$, of a coin flip\n",
    "-----\n",
    "\n",
    "Any guesses for a prior probability of coin flip?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's use Beta(Î±,Î²) distribution "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Beta(Î±,Î²) distribution \n",
    "-----\n",
    "\n",
    "<center><img src=\"http://www.epixanalytics.com/modelassist/AtRisk/images/12/image19.gif\" height=\"500\"/></center>\n",
    "\n",
    "\n",
    "The Î² family of distributions is a gold mine because it allows for a wide variety of distributions using __two__ input parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A Î² distribution can represent a distribution of probabilities - all the possible values of a probability when we do __not__ know what that probability is.\n",
    "\n",
    "For example, whether a coin is fair or weighted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A Î² distribution is easy to update\n",
    "--------\n",
    "\n",
    "We can start with a guess, then collect more data and make a new Beta distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For example, flip a coin 25 more times and count the number of heads and tails:\n",
    "\n",
    "Beta(Î±<sub>0</sub>+heads, Î²<sub>0</sub>+tails)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "[Source](https://stats.stackexchange.com/questions/47771/what-is-the-intuition-behind-beta-distribution)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can maximize the posterior function, $\\mathbb{P}(\\theta|x)$. \n",
    "\n",
    "Because the logarithm is convex, we can use it to make the maximization process easier by converting the product to a sum without changing the extrema that we are looking for. \n",
    "\n",
    "We rather work with the logarithm of $\\mathbb{P}(\\theta|x)$ as in the following.\n",
    "\n",
    "$$\n",
    "\\mathcal{L} := \\log \\mathbb{P}(\\theta|x) =  \\log \\ell(\\theta) + \\log\\mathbb{P}(\\theta) - \\log\\mathbb{P}(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- Maximum A Posteriori (MAP) estimation takes into account P(Î˜) / Prior it its estimation.\n",
    "- Maximum Likelihood Estimation (MLE) only uses the current data to make a guess (inference) about the parameter.\n",
    "- MAP uses beliefs or historical data to adjust parameter estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bonus Material\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
