{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.arima_model import ARIMA, ARIMAResults\n",
    "from statsmodels.tsa.arima_process import ArmaProcess\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from scipy import signal\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Always make it pretty.\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# We get some interger -> float typecast warning from sklean below, this keeps them out of our hair.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "this_directory = !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_directory[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_directory = !pwd\n",
    "os.chdir(this_directory[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series - Fundamental Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A *time series* is a specific type of data, where measurements of a single quantity are taken over time.\n",
    "\n",
    "When speaking in equations, we will generally represent time with an index $i$, and the observations from the series as $y_i$.  So the time series as a whole is\n",
    "\n",
    "$$y_1, y_2, y_3, \\ldots $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "A good source for time series data is [google trends](https://www.google.com/trends/) where you can find how the popularity of a search term varies over time:\n",
    "\n",
    "We've provided some chosen google trends in the `data` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_google_trend_data(name):\n",
    "    \"\"\"Load saved data for a google trend.\n",
    "    \n",
    "    NOTE: For this to work you need to have started your notebook from the\n",
    "    lecture directory.\n",
    "    \"\"\"\n",
    "    file_name = os.path.join('.', 'data', ''.join(['search-index-', name, '.txt']))\n",
    "    df = pd.read_csv(file_name)\n",
    "    df = df.set_index(pd.DatetimeIndex(df.week))\n",
    "    del df['week']\n",
    "    return pd.Series(df[name], df.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot these trends and look for some interesting patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trend_data(ax, name, series):\n",
    "    ax.plot(series.index, series)\n",
    "    ax.set_title(\"Google Search Trend For {}\".format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_search_names = ['baseball', 'python', 'pokemon', 'taxes', 'gdp', 'gmail', 'blackberry']\n",
    "\n",
    "google_trends = {\n",
    "    name: load_google_trend_data(name)\n",
    "    for name in google_search_names\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(len(google_trends), figsize=(14, 12))\n",
    "\n",
    "for ax, name in zip(axs, google_search_names):\n",
    "    plot_trend_data(ax, name, load_google_trend_data(name))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some observations:\n",
    "\n",
    "  - Searches for **baseball** show very strong oscillations aligned with the baseball season.  Search volume increases quickly as the season begins, and then falls off as the season goes on.\n",
    "  - Searches for **python** show a general increase over time, probably influenced by the popularity of the language itself.  There may be evidence that searches decrease as specific periods of the year, most noticeably the end of year, when people are not working.\n",
    "  - Searches for **pokemon** spike suddenly at a recent date, pointing to a one-time event greatly increasing interest.\n",
    "  - Searches for **taxes** spike regularly at two specific points of the year, with the interest being heightened between those two dates.  At other times of the year, searches revert to a low rate.\n",
    "  - Searches for **gdp** show a more random pattern than the others.  There is some evidence for *stickiness* in the trend, low values beget low subsequent values, and high values beget high subsequent values.\n",
    "  - Searches for **gmail** show a pattern reminiscent of python, but there are sudden shifts where the baseline level jumps to a new value.\n",
    "  - Searches for **blackberry** have a non-linear trend over time, they increase to a peak popularity, then decrease to almost nothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity:** Load and plot the trend data for \"data-science\", what patterns do you see?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14,2))\n",
    "ax = plt.subplot(111)\n",
    "plot_trend_data(ax, 'data-science', load_google_trend_data('data-science'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various different time series often show common patterns.  Attaching words to these patterns allows us to build a common language to discuss time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trends\n",
    "\n",
    "A **trend** in a time series is a gradual change in average level as time moves on.  A trend an be *increasing*, *decreasing*, or *neither* (if, for example, a trend changes direction at some point in time). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searches for **python** show a steady increasing trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 2))\n",
    "plot_trend_data(ax, 'python', google_trends['python'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Searches for **blackberry** show a non-linear trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 2))\n",
    "plot_trend_data(ax, 'blackberry', google_trends['blackberry'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How you you describe the trend in `data-science`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can often use a regression model to capture a general trend in the series.\n",
    "\n",
    "Let's try to capture the linear trend in the python search data by fitting a simple linear model and using its predictions on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_col_vector(arr):\n",
    "    \"\"\"Convert a one dimensional numpy array to a column vector.\"\"\"\n",
    "    return arr.reshape(-1, 1)\n",
    "\n",
    "def make_design_matrix(arr):\n",
    "    \"\"\"Construct a design matrix from a numpy array, including an intercept term.\"\"\"\n",
    "    return sm.add_constant(to_col_vector(arr), prepend=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_linear_trend(series):\n",
    "    \"\"\"Fit a linear trend to a time series.  Return the fit trend as a numpy array.\"\"\"\n",
    "    X = make_design_matrix(np.arange(len(series)) + 1)\n",
    "    linear_trend_ols = sm.OLS(series.values, X).fit()\n",
    "    linear_trend = linear_trend_ols.predict(X)\n",
    "    return linear_trend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_linear_trend(ax, name, series):\n",
    "    linear_trend = fit_linear_trend(series)\n",
    "    plot_trend_data(ax, name, series)\n",
    "    ax.plot(series.index, linear_trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 2))\n",
    "plot_linear_trend(ax, 'python', google_trends['python'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we subtract out the fit trend from the original series, we get the **detrended series**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_series = google_trends['python']\n",
    "python_linear_trend = fit_linear_trend(python_series)\n",
    "python_series_detrended = python_series - python_linear_trend\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(14, 2))\n",
    "ax.plot(python_series_detrended.index, python_series_detrended)\n",
    "ax.set_title(\"Google Searches for Python, Detrended\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detrending a series is often times a first step in analysing a time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many times time series show a more sophisticated trend than a simple linear increase or decrease, in these cases a more sophisticated detrending procedure is needed.\n",
    "\n",
    "In the following two cases, a linear detrending would be clearly inappropriate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(14, 4))\n",
    "plot_linear_trend(axs[0], 'blackberry', google_trends['blackberry'])\n",
    "plot_linear_trend(axs[1], 'gmail', google_trends['gmail'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Can you describe preciesely what is wrong with the linear model in both of these cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very general approach that can be used for detrending data is to compute a **moving average**.\n",
    "\n",
    "The moving average estimate of the trend at a data point $y_i$ is\n",
    "\n",
    "$$ \\hat y_i = \\frac{1}{2w + 1} \\sum_{j = -w}^{w} y_{i + j} $$\n",
    "\n",
    "We esentially slide a *window* of a fixed side across our data, and average the values of the series within the window.\n",
    "\n",
    "The parameter $w$ controls how far to the left and to the right of $w_i$ we look when averaging the nearby points, this is called the **window**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_moving_average_trend(series, window=6):\n",
    "    return pd.rolling_mean(series, window, center=True)\n",
    "\n",
    "def plot_moving_average_trend(ax, name, series, window=6):\n",
    "    moving_average_trend = fit_moving_average_trend(series, window)\n",
    "    plot_trend_data(ax, name, series)\n",
    "    ax.plot(series.index, moving_average_trend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, figsize=(14, 6))\n",
    "plot_moving_average_trend(axs[0], 'python', google_trends['python'])\n",
    "plot_moving_average_trend(axs[1], 'blackberry', google_trends['blackberry'])\n",
    "plot_moving_average_trend(axs[2], 'gmail', google_trends['gmail'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Varying the window parameter changes the quality of the moving average fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(6, figsize=(14, 12))\n",
    "\n",
    "gmail_series = google_trends['gmail']\n",
    "for i, window in enumerate([4, 8, 12, 16, 20, 24]):\n",
    "    axs[i].plot(gmail_series.index, gmail_series)\n",
    "    axs[i].plot(gmail_series.index, fit_moving_average_trend(gmail_series, window=window))\n",
    "    axs[i].set_title(\"Moving Average Smoothed Gmail Search Data with window = {} weeks\".format(window))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that:\n",
    "\n",
    "  - Smaller values of `window` will tend to be influenced by noise of other non-trend patterns in the series.\n",
    "  - Large values of `window` produce smoother estimates of the general trend in the data.\n",
    "  \n",
    "For this reason, when estimating the trend component of a time series, we generally prefer larger windows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity:** Fit a moving average smooth to the data science series for various windows, and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we have data that aligns with calendar regularities (quarterly, weekly, yearly), it is a good idea to chose the window so that an entire annual cycle is used in the smooth.  This will average out any **seasonal** patterns in the data, as we will discuss below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(3, figsize=(14, 6))\n",
    "plot_moving_average_trend(axs[0], 'python', google_trends['python'], window=52)\n",
    "plot_moving_average_trend(axs[1], 'blackberry', google_trends['blackberry'], window=12)\n",
    "plot_moving_average_trend(axs[2], 'gmail', google_trends['gmail'], window=52)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonality\n",
    "\n",
    "A **seasonal** pattern in a time series is one that tends to appear regularly, and aligns with features of the calendar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(14, 4))\n",
    "plot_trend_data(axs[0], 'baseball', google_trends['baseball'])\n",
    "plot_trend_data(axs[1], 'taxes', google_trends['taxes'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like we can de**trend** a time series, we can also **deseasonalize** a time series.\n",
    "\n",
    "The simplest method is to create dummy variables at regular intervals of the calender\n",
    "  - A dummy for each month.\n",
    "  - A dummy for each season.\n",
    "  \n",
    "and then fit a linear regression to the series using these dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_monthly_dummies(series):\n",
    "    month = series.index.month\n",
    "    # Only take 11 of the 12 dummies to avoid strict colinearity.\n",
    "    return pd.get_dummies(month).ix[:, :11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_seasonal_trend(series):\n",
    "    dummies = create_monthly_dummies(series)\n",
    "    X = sm.add_constant(dummies.values, prepend=False)\n",
    "    seasonal_model = sm.OLS(series.values, X).fit()\n",
    "    return seasonal_model.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_seasonal_trend(ax, name, series):\n",
    "    seasons_average_trend = fit_seasonal_trend(series)\n",
    "    plot_trend_data(ax, name, series)\n",
    "    ax.plot(series.index, seasons_average_trend)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are linear regression models fit to the baseball and taxes trends with monthly dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(14, 5))\n",
    "\n",
    "plot_seasonal_trend(axs[0], 'baseball', google_trends['baseball'])\n",
    "plot_seasonal_trend(axs[1], 'taxes', google_trends['taxes'])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To deseasonalize, we simply subtract out the seasonal predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball_series = google_trends['baseball']\n",
    "baseball_seasonal_trend = fit_seasonal_trend(baseball_series)\n",
    "baseball_seasonal_detrended = baseball_series - baseball_seasonal_trend\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(14, 2))\n",
    "ax.plot(baseball_series.index, baseball_seasonal_detrended)\n",
    "ax.set_title(\"Google Searches for Baseball, Deseasonalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball_series = google_trends['taxes']\n",
    "baseball_seasonal_trend = fit_seasonal_trend(baseball_series)\n",
    "baseball_seasonal_detrended = baseball_series - baseball_seasonal_trend\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(14, 2))\n",
    "ax.plot(baseball_series.index, baseball_seasonal_detrended)\n",
    "ax.set_title(\"Google Searches for Taxes, Deseasonalized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Have we removed the seasonality from these series?  What more could we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity:** Deseasonalize the `taxes` series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trend-Seasonal-Residual Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Classical Trend-Seasonal-Residual Decomposition** expresses a time series as the sum of three components:\n",
    "\n",
    "$$ y_t = T_t + S_t + R_t $$\n",
    "\n",
    "and is accomplished as follows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose, for definiteness, that we are working with *weekly* data, so that each $52$ observations forms a calender year.  Then, the series is decomposed as follows:\n",
    "\n",
    "1. Compute the trend component $T_t$ using a moving average with window width $52$ (or $12$ for monthly data.  Then detrend the series.\n",
    "2. Compute the seasonal component $S_t$ of the detrended series $y_t - T_t$ by averaging together the observations that fall in the same week (or month, if monthly data).  **Note**, this is *equivalent* to fitting a linear regression to the detrended data with an indicator for each week, and then making predictions for each week of the original series.\n",
    "3. The **remainder**, or **error**, or **residual** series $E_t$ is $y_t - T_t - S_t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statsmodels implements the classical decomposition as `seasonal_decompose`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_decomposition = sm.tsa.seasonal_decompose(google_trends['python'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_seasonal_decomposition(axs, series, sd):\n",
    "    axs[0].plot(series.index, series)\n",
    "    axs[0].set_title(\"Raw Series\")\n",
    "    axs[1].plot(series.index, sd.trend)\n",
    "    axs[1].set_title(\"Trend Component $T_t$\")\n",
    "    axs[2].plot(series.index, sd.seasonal)\n",
    "    axs[2].set_title(\"Seasonal Component $S_t$\")\n",
    "    axs[3].plot(series.index, sd.resid)\n",
    "    axs[3].set_title(\"Residual Component $R_t$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, figsize=(14, 8))\n",
    "plot_seasonal_decomposition(axs, google_trends['python'], python_decomposition)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This decomposition validates our description of the series earlier as a general upwards trend, but with some seasonal behaviour (like a sharp drop off in queries around wintertime)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the decomposition is working well, the residual component $R_t$ should show no seasonal or trend patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball_decomposition = sm.tsa.seasonal_decompose(google_trends['baseball'])\n",
    "\n",
    "fig, axs = plt.subplots(4, figsize=(14, 8))\n",
    "plot_seasonal_decomposition(axs, google_trends['baseball'], baseball_decomposition)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The different components of the series are often shown on very different scales, so make sure to pay close attention to the $y$-axis labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the classical decomposition is not powerful enough to capture the trend or seasonal components of a time series.\n",
    "\n",
    "The gmail series decomposition shows some interesting behaviour in the residuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gmail_decomposition = sm.tsa.seasonal_decompose(google_trends['gmail'])\n",
    "\n",
    "fig, axs = plt.subplots(4, figsize=(14, 8))\n",
    "plot_seasonal_decomposition(axs, google_trends['gmail'], gmail_decomposition)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around October 2013 the series showed a clear and sudden change in average level, which the trend component could not adapt to quickly enough.  Consequently, this discontinuity appears in the residual series.\n",
    "\n",
    "A more sophisticated procedure should be used to decompose this series, which allows for a discontinuity in the observed series, or fits a more sophisticated model for the trend component."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity:** Decompose the data science series.  What patterns do you see?  Is the decomposition appropriate?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical Concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the second part we will present some statistical models for time series.  For now, we will present some fundamental concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up to now we have only considered a single manifestation of a single time series\n",
    "\n",
    "$$ y_1, y_2, y_3, \\ldots $$\n",
    "\n",
    "Let's now impose a probability model on this data, i.e., now we consider series as **random objects that can be sampled**.\n",
    "\n",
    "Formally, we consider a single time series as a *sample* from a sequence of random variables\n",
    "\n",
    "$$ Y_1, Y_2, Y_3, \\ldots $$\n",
    "\n",
    "Note that we are **not** assuming that these random variables are independent, **it is very likely that in a time series that sample we draw at time $t$ influences the sample we draw at time $t+1$**.\n",
    "\n",
    "When we need to distinguish the data from the statistical model that we are assuming generated the data, we call:\n",
    "\n",
    "  - The data a **series**.\n",
    "  - The data generating process a **random process**, or more specifically, a **time series random process**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### White Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest possible random process from this perspective occurs when each $Y_i$ is independent from all the rest, and all the $Y$'s are identically distributed.\n",
    "\n",
    "In this case the series is called **white noise**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def white_noise(size=250):\n",
    "    return pd.Series(np.random.normal(size=size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, figsize=(14, 8))\n",
    "for i in range(4):\n",
    "    noise = white_noise()\n",
    "    axs[i].plot(noise.index, noise)\n",
    "    if i == 0:\n",
    "        axs[i].set_title(\"White Noise Series\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "White noise shows no trend, no seasonal patterns, no cyclic behaviour, and no stickiness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "White noise has the simplest possible structure, everything is independent.\n",
    "\n",
    "As mentioned, in most time series observations will **not** be independent.  Nonetheless, there is a useful concept that characterizes the type of dependence found in many time series.\n",
    "\n",
    "A time series random process $Y_1, Y_2, Y_3, \\ldots$ is said to be **strictly stationary** if the distribution of all equally spaced tuples taken from the series are identical.  That is, if\n",
    "\n",
    "$$ (Y_{i_1}, Y_{i_2}, \\ldots, Y_{i_k}) $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ (Y_{i_1 + k}, Y_{i_2 + k}, \\ldots, Y_{i_3 + k}) $$\n",
    "\n",
    "are always identically distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This means that** the properties of the series **do not** depend on when you begin observing it.\n",
    "\n",
    "**This means that** if you start observing the series, watch for an hour, go cook a meal for an hour, then watch the series for another hour, **it does not matter if the meal you cooked was breakfast, lunch, or dinner** the properties of the series you observe will be the same in any case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Can stationary series show trends?  Can stationary series show seasonality?  Can stationary series be sticky?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Can the variance of a stationary series change over time?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Is the following series stationary? No? Summarize all the reasons why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_name = os.path.join('.', 'data', 'jj.txt')\n",
    "df = pd.read_csv(file_name)\n",
    "earnings_series = pd.Series(df.earnings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 4))\n",
    "ax.plot(earnings_series.index, earnings_series)\n",
    "ax.set_title(\"Company Earnings Over Time\")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** How would you transform this series to be stationary?\n",
    "  - How would you remove the trend?\n",
    "  - How would you stabilize the variance?  [This may help](http://stats.stackexchange.com/questions/18930/transformation-for-stabilizing-variance-in-time-series)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differencing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most series encountered in nature are **not** stationary, but there is often a simple way to transform one to be stationary.\n",
    "\n",
    "Given a time series $y_1, y_2, y_3, \\ldots$, the **first differences series** is\n",
    "\n",
    "$$ y_2 - y_1, y_3 - y_2, y_4 - y_3, \\ldots $$\n",
    "\n",
    "The aim of differencing a series is to remove the trend component, which stationary series cannot have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_series_and_difference(axs, series, title):\n",
    "    diff = series.diff()\n",
    "    axs[0].plot(series.index, series)\n",
    "    axs[0].set_title(\"Raw Series: {}\".format(title))\n",
    "    axs[1].plot(series.index, diff)\n",
    "    axs[1].set_title(\"Series of First Differences: {}\".format(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(14, 4))\n",
    "plot_series_and_difference(axs, google_trends['python'], 'python')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(14, 4))\n",
    "plot_series_and_difference(axs, google_trends['gmail'], 'gmail')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Do these difference series look stationary?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity:** Compute and plot the first differences series for data science.  Does it look stationary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** If you are given the *differences* of a series, how can you recover the original series?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Random Walks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When a differenced series is *white noise* (**note**: *not all stationary series are white noise*, as we will demonstrate below) we have the following relationship\n",
    "\n",
    "$$ y_{i+1} - y_i = e_i $$\n",
    "\n",
    "or \n",
    "\n",
    "$$ y_{i+1} = y_i + e_i $$\n",
    "\n",
    "This type of series is called a **random walk** and is a very common statistical model for time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_walk(size=250):\n",
    "    noise = np.random.normal(size=(size + 1))\n",
    "    walk = np.cumsum(noise)\n",
    "    return pd.Series(walk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_random_walk(ax, size=250):\n",
    "    walk = random_walk(size=size)\n",
    "    ax.plot(walk.index, walk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, figsize=(14, 8))\n",
    "for i in range(4):\n",
    "    walk = random_walk()\n",
    "    diff = walk.diff()\n",
    "    if i%2 == 0:\n",
    "        axs[i].plot(walk.index, walk)\n",
    "        axs[i].set_title(\" Random Walk Series\")\n",
    "    if i%2 == 1:\n",
    "        axs[i].plot(diff.index, diff)\n",
    "        axs[i].set_title(\"Differenced Random Walk Series\") \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that random walks, although constructed from totally random components, can show very strong evidence of *trends*.  This is because random walks are sticky, once they randomly elevate to an extreme value, subsequent values of the series are only small adjustments to previous values, so the value of the series tends to stay elevated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Is the differenced series from a random walk white noise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Linear Combinations of White Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way to construct a diverse set of stationary series is as **linear combinations of white noise**.\n",
    "\n",
    "Suppose that \n",
    "\n",
    "$$ e_1, e_2, e_3, \\ldots $$\n",
    "\n",
    "is a white noise process (what was that again?).  Then any series created by linearly combining lagged values of $e$ is stationary\n",
    "\n",
    "$$ s_i = \\mu + e_i + a_1 e_{i-1} + a_2 e_{i-2} + \\cdots + a_k e_{i-k} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_combination_of_white_noise(size, coef):\n",
    "    coef = np.asarray(coef)\n",
    "    n_coef = len(coef)\n",
    "    noise_size = size + len(coef)\n",
    "    noise = np.random.normal(size=noise_size)\n",
    "    # np.convolve reverses the second array :/\n",
    "    # We need to subset the result to remove edge effects.\n",
    "    lc = np.convolve(noise, coef[::-1])[(n_coef-1):(size+n_coef-1)]\n",
    "    return pd.Series(lc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_linear_combination_of_white_noise(ax, size, coef):\n",
    "    series = linear_combination_of_white_noise(size, coef)\n",
    "    ax.plot(series.index, series)\n",
    "    ax.set_title(\"Linear Combination of White Noise with Coefficients {}\".format(\n",
    "            str(list(coef[1:]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, figsize=(14, 8))\n",
    "plot_linear_combination_of_white_noise(axs[0], 250, [1, 1])\n",
    "plot_linear_combination_of_white_noise(axs[1], 250, [1, -1])\n",
    "plot_linear_combination_of_white_noise(axs[2], 250, [1, 1, 1, 1, 1])\n",
    "plot_linear_combination_of_white_noise(axs[3], 250, [1, 0.5, 0.25, 0.1])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**All the series above are sampled from a stationary process.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A linear combination of white noise is traditionally called a **MA series**.\n",
    "\n",
    "  - An MA(1) contains one lagged term: $s_i = \\mu + e_i + a_1 e_{i-1}$\n",
    "  - An MA(2) contains two lagged terms: $s_i = \\mu + e_i + a_1 e_{i-1} + a_2 e_{i-2}$\n",
    "  - And so on...\n",
    "  \n",
    "**Note:**  MA stands for *moving average*, even though this has *nothing to do with* a moving average as we discussed earlier.  Unfortunately, the name has stuck, and you'll have to get use to it.  My recommendation is to call them \"em ay\" series, and reserve the full words for true moving averages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity:** Take a few minuets to play around with generating some MA series.  Can you create any interesting behaviour out of pure randomness?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests for Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stationarity can be hard to visualize, so there are some statistical hypothesis tests that you can use to indicate stationarity.\n",
    "\n",
    "The **Augmented Dickey-Fuller test** is setup as follows:\n",
    "\n",
    "  - $H_0$: The series is **not-stationary**.\n",
    "  - $H_{\\text{a}}$: The series is stationary.\n",
    "  \n",
    "It is available in stats models as `tsa.stattools.adfuller`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "White noise series are stationary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    test = sm.tsa.stattools.adfuller(white_noise(size=250))\n",
    "    print(\"ADF p-value: {0:2.2f}\".format(test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MA series are stationary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coef in [[1, 1], [1, -1], [1, 1, 1, 1], [1, 0.5, 0.25, 0.1]]:\n",
    "    series = linear_combination_of_white_noise(size=250, coef=coef)\n",
    "    test = sm.tsa.stattools.adfuller(series)\n",
    "    print(\"ADF p-value: {0:2.2f}\".format(test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about some of the series we found in nature?\n",
    "\n",
    "The python series has an obvious trend, it is clearly not stationary, and the test confirms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sm.tsa.stattools.adfuller(google_trends['python'])\n",
    "print(\"ADF p-value for python series: {0:2.2f}\".format(test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe differencing helped?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sm.tsa.stattools.adfuller(google_trends['python'].diff()[1:])\n",
    "print(\"ADF p-value for differenced python series: {0:2.2f}\".format(test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **It sure did!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The baseball series is also stationary after differencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sm.tsa.stattools.adfuller(google_trends['baseball'].diff()[1:])\n",
    "print(\"ADF p-value for differenced baseball series: {0:2.2f}\".format(test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity:** Test some of the other series we've been experimenting with for stationality.  Does differencing improve the situation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = sm.tsa.stattools.adfuller(google_trends['gmail'].diff()[1:])\n",
    "print(\"ADF p-value for differenced gmail series: {0:2.2f}\".format(test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Final Activity:** Can you transform the `earnings` series from earlier to be stationary?  There is a step you will have to take that we have **not** discussed.  We will begin the next section by discussing this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Work goes here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series - Models for Stationary Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of the day, we will be working with *stationary* series, and developing a statistical model for them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the definition of strictly stationary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A time series $Y_1, Y_2, Y_3, \\ldots$ is said to be **strictly stationary** if the distribution of all equally spaced tuples taken from the series are identical.  That is, if\n",
    "\n",
    "$$ (Y_{i_1}, Y_{i_2}, \\ldots, Y_{i_k}) $$\n",
    "\n",
    "and\n",
    "\n",
    "$$ (Y_{i_1 + k}, Y_{i_2 + k}, \\ldots, Y_{i_3 + k}) $$\n",
    "\n",
    "are always identically distributed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One consequence of this definition is that the correlation between two $Y$'s only depends on the lag between them:\n",
    "\n",
    "$$ Corr(Y_5, Y_2) = Corr(Y_4, Y_1) = Corr(Y_{144}, Y_{141}) = \\cdots $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These correlations between a time series and a lagged version of itself are called **autocorrelations**.\n",
    "\n",
    "$$ \\gamma_k(Y)= Corr(Y_1, Y_{1+k}) = Corr(Y_2, Y_{2+k}) = Corr(Y_3, Y_{3+k}) = \\cdots $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def series_and_lagged(series, lag=1):\n",
    "    truncated = np.copy(series)[lag:]\n",
    "    lagged = np.copy(series)[:(len(truncated))]\n",
    "    return truncated, lagged\n",
    "\n",
    "def compute_autocorrelation(series, lag=1):\n",
    "    series, lagged = series_and_lagged(series, lag=lag)\n",
    "    return np.corrcoef(series, lagged)[0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from earlier that the differenced baseball series **is** stationary (according to the ADF test)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(14, 4))\n",
    "plot_series_and_difference(axs, google_trends['baseball'], 'baseball')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can draw some scatterplots to summarize the autocorrelation information in the baseball series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseball_diff = google_trends['baseball'].diff()[1:]\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(8, 8))\n",
    "\n",
    "for i, ax in enumerate(axs.flatten()):\n",
    "    series, lagged = series_and_lagged(baseball_diff, lag=i)\n",
    "    autocorr = compute_autocorrelation(baseball_diff, lag=i)\n",
    "    ax.scatter(series, lagged, alpha=0.5)\n",
    "    ax.set_title(\"Lag {0} AC: {1:2.2f}\".format(i, autocorr))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why does the first scatterplot here lie along a straight line?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A more immediate way to view the autocorrelation in a series is with an **autocorrelation plot**.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(16, 4))\n",
    "\n",
    "_ = sm.graphics.tsa.plot_acf(baseball_diff, lags=2*52, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each spike in this plot is an autocorrelation for a single lag.\n",
    "\n",
    "The *first* spike is always at $1.0$ (because any series is perfectly correlated with itself.\n",
    "\n",
    "The shaded region are 95% confidence bounds.  If *all* of the autocorrelations were truly zero, we would still expect to see about 5% of the spikes exceeding these bounds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the baseball example, we observe the following:\n",
    "  - There is a very striking spike at exactly 52 lags, which is the number of weeks in a year.  This is repeated, less significantly, at a lag of twice 52.\n",
    "  - The first and 53'rd autocorrelations are positive and significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** How do you interpret the large spike in autocorrelation at $52$ weeks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Could the spike at $104$ weeks be related to the spike at $52$ weeks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Does the spike at 52 weeks *prevent the series from being stationary*?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One potential issue with autocorrelations is the following:\n",
    "    \n",
    "If $y_i$ is correlated with $y_{i-1}$ **and** $y_{i-1}$ is correlated with $y_{i-2}$ then $y_{i}$ **necessarily** has some influence on $y_{i-2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial Autocorrelation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **partial autocorrelation** plot controls for this effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(16, 4))\n",
    "\n",
    "_ = sm.graphics.tsa.plot_pacf(baseball_diff, lags=2*52, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the spike at $52$ weeks still appears, but the subsequent spike at $104$ weeks has disappeared.  This demonstrates that the spike at $104$ weeks was almost completely caused by the spike at $52$ weeks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** How do you think the partial autocorrelation plot is constructed.  **Hint**: It uses linear regression in a fundamental way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** What would you expect from an autocorrelation plot of white noise?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autocorrelation of MA series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that MA series are linear combinations of white noise with a possible constant term\n",
    "\n",
    "$$ s_i = \\mu + e_i + a_1 e_{i-1} + \\cdots + a_k e_{i-k} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, figsize=(14, 8))\n",
    "\n",
    "ma_coefs = [[1, 0.5], [1, -0.5], [1, 0.5, 0.5], [1, -0.5, 0.5, -0.25, 0.25]]\n",
    "for i, (ax, coef) in enumerate(zip(axs, ma_coefs)):\n",
    "    plot_linear_combination_of_white_noise(axs[i], 250, coef)\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** What will the autocorrelation of a MA series look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, figsize=(14, 8))\n",
    "\n",
    "for i, (ax, coef) in enumerate(zip(axs, ma_coefs)):\n",
    "    series = linear_combination_of_white_noise(size=250, coef=coef)\n",
    "    _ = sm.graphics.tsa.plot_acf(series, lags=25, ax=ax)\n",
    "    ax.set_title(\"Autocorrelation in MA With Coefficients {}\".format(str(list(coef[1:]))))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For data generated from an MA model, the number of non-zero autocorrelations generally indicates the *order* of the model (the number of non-zero coefficients)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MA Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given stationary time series data, a reasonable question to ask is the following:\n",
    "    \n",
    "> Which MA series would be most likely to generate this data?\"\n",
    "\n",
    "This is the concept behind MA models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A MA (moving average) model of order $k$ (also known as a $MA(k)$ model) attempts to represent a stationary time series as a linear combination of white noise:\n",
    "\n",
    "$$ s_i = \\mu + e_i + a_1 e_{i-1} + \\cdots + a_k e_{i-k} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** What technique do you think is used to determine the estimated coefficients when an MA model is fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_ma_model(series, order):\n",
    "    # ARIMA is a more general model, which we are going to build up\n",
    "    # in pieces.\n",
    "    model = ARIMA(series, order=(0, 0, order))\n",
    "    return model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some MA data, fit MA models, and compare the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def format_list_of_floats(L):\n",
    "    return [\"{0:2.2f}\".format(f) for f in L]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coef in ma_coefs:\n",
    "    ma_series = linear_combination_of_white_noise(size=250, coef=coef)\n",
    "    ma_model = fit_ma_model(ma_series.values, order=(len(coef) - 1))\n",
    "    print(\"True Coefficients: {0: <20} Estimated Coefficients: {1: <20}\".format(\n",
    "            str(format_list_of_floats(coef[1:])),\n",
    "            str(format_list_of_floats(ma_model.params[1:]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So it looks like the MA models recover the true coefficients from *actual* MA processes.  So far, so good.\n",
    "\n",
    "Before we apply the MA model to real data, we need to introduce AR processes, and generalize everything to the ARIMA model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intermission: Recursive Sequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **recursive sequence** (of numbers) is one where the subsequent values in the sequence are defined as functions of the preceding values.  For example:\n",
    "\n",
    "  - $y_i = 1$\n",
    "  - $y_i = y_{i-1} + 1; \\ y_1 = 0$\n",
    "  - $y_i = - y_{i-1}; \\ y_1 = 1$\n",
    "  - $y_i = 0.5 y_{i-1} + 0.25 y_{i-2}; \\ y_1 = 1; y_2 = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Activity:** Sketch pictures of these recursive sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bonus:**: Write a python function to draw a plot of a recursive sequence.  How should you specify the arguments to this function?  How can you factor the task into multiple functions so that each has a single responsibility?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR Processes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another data generating process that (often, there are some restrictions on the coefficients) results in a stationary process is an **autoregressive** process.\n",
    "\n",
    "In an autoregressive process, subsequent values of the series are linear combinations of previous values of the series plus some noise term (much like in regression we get a linear combination of predictors plus a noise term)\n",
    "\n",
    "$$ y_i = \\mu + b_1 y_{i-1} + b_2 y_{i-2} + \\cdots + b_k y_{i-k} + \\epsilon_i $$\n",
    "\n",
    "The number of coefficients in this equation is called the *order* of the model, and we often speak of $AR(1)$ processes, $AR(2)$ processes, etc..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def auto_regressive_process(size, coefs, init=None):\n",
    "    \"\"\"Generate an autoregressive process with Gaussian white noise.  The\n",
    "    implementation is taken from here:\n",
    "    \n",
    "      http://numpy-discussion.10968.n7.nabble.com/simulate-AR-td8236.html\n",
    "      \n",
    "    Exaclty how lfilter works here takes some pen and paper effort.\n",
    "    \"\"\"\n",
    "    coefs = np.asarray(coefs)\n",
    "    if init == None:\n",
    "        init = np.array([0]*len(coef))\n",
    "    else:\n",
    "        init = np.asarray(init)\n",
    "    init = np.append(init, np.random.normal(size=(size - len(init))))\n",
    "    assert(len(init) == size)\n",
    "    a = np.append(np.array([1]), -coefs)\n",
    "    b = np.array([1])\n",
    "    return pd.Series(signal.lfilter(b, a, init))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_autoregressive_process(ax, size, coefs, init=None):\n",
    "    ar = auto_regressive_process(size, coefs, init)\n",
    "    ax.plot(ar.index, ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar_coefs = [[0.5], [-0.5], [1.5, -0.5], [1.5, -1, 0.25]]\n",
    "\n",
    "fig, axs = plt.subplots(4, figsize=(14, 8))\n",
    "\n",
    "for i, (ax, coefs) in enumerate(zip(axs, ar_coefs)):\n",
    "    plot_autoregressive_process(ax, 250, coefs)\n",
    "    ax.set_title(\"Autoregressive Process with Coefficients {}\".format(str(list(coefs))))\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Discussion:** What will the autocorrelation of an AR series look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, figsize=(14, 8))\n",
    "\n",
    "for i, (ax, coef) in enumerate(zip(axs, ar_coefs)):\n",
    "    series = auto_regressive_process(size=250, coefs=coef)\n",
    "    _ = sm.graphics.tsa.plot_acf(series, lags=25, ax=ax)\n",
    "    ax.set_title(\"Autocorrelation in AR With Coefficients {}\".format(str(list(coef))))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In an AR process, even thought subsequent data points are only *directly* influenced by a few previous data points, the influence tends to linger for a long time.\n",
    "\n",
    "We can see this lingering influence in:\n",
    "  - The gradual decrease in autocorrelation when the initial coefficient is positive.\n",
    "  - The sinusoidal behaviour when there is negative coefficients.\n",
    "\n",
    "For AR processes, controlling for this lingering influence with a *partial* autocorrelation plot is important."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(4, figsize=(14, 8))\n",
    "\n",
    "for i, (ax, coef) in enumerate(zip(axs, ar_coefs)):\n",
    "    series = auto_regressive_process(size=250, coefs=coef)\n",
    "    _ = sm.graphics.tsa.plot_pacf(series, lags=25, ax=ax)\n",
    "    ax.set_title(\"Partial Autocorrelation in AR With Coefficients {}\".format(str(list(coef))))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the partial autocorrelation plot, we see very distinctly that the number of significant spikes is directly influenced by the number of non-zero coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AR Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given stationary time series data, a reasonable question to ask is the following:\n",
    "    \n",
    "> Which AR series would be most likely to generate this data?\n",
    "\n",
    "This is the concept behind AR models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_ar_model(series, order):\n",
    "    # ARIMA is a more general model, which we are going to build up\n",
    "    # in pieces.\n",
    "    model = ARIMA(series, order=(order, 0, 0))\n",
    "    return model.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's generate some AR data, fit some AR models, then compare the coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for coef in ar_coefs:\n",
    "    ar_series = auto_regressive_process(size=250, coefs=coef)\n",
    "    ar_model = fit_ar_model(ar_series.values, order=(len(coef)))\n",
    "    print(\"True Coefficients: {0: <26} Estimated Coefficients: {1: <26}\".format(\n",
    "            str(format_list_of_floats(coef)),\n",
    "            str(format_list_of_floats(ar_model.params[1:]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like for MA models, it looks like out AR models recover the true coefficients from an AR process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARMA Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have discussed AR and MA models, we can *combine* them into ARMA models, which have both AR and MA components.\n",
    "\n",
    "ARMA models have both:\n",
    "  - A **linear combination of white noise** component (the MA), where subsequent values of the series are (partially) a linear combination of white noise.\n",
    "  - An **autoregressive component** the (AR), where subsequent values of the series are (partially) a linear combination of previous terms, plus noise.\n",
    "\n",
    "  \n",
    "$$ s_i = \\mu + \\overbrace{e_i + a_1 e_{i-1} + \\cdots + a_k e_{i-k}}^{\\text{MA component}} + \\overbrace{b_1 y_{i-1} + b_2 y_{i-2} + \\cdots + b_k y_{i-k} + \\epsilon_i}^{\\text{AR component}} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARIMA Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **I** in ARIMA stands for **integrated**.\n",
    "\n",
    "The word *integration* is used here as the opposite of *difference*.\n",
    "\n",
    "**Recall:** We often have to take the *first differences* of series we find in nature to make then *stationary*.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(14, 4))\n",
    "plot_series_and_difference(axs, google_trends['python'], 'python')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MA and AR (and, consequently ARMA) models *only apply to* stationary series.\n",
    "\n",
    "So to model series in nature, we often have to model the *differenced* series as an ARMA process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This leads us, finally, to **ARIMA**.\n",
    "\n",
    "An **ARIMA(p, d, q)** model:\n",
    "  - Applies an ARMA model to a series that has been differenced $d$ times.\n",
    "  - The AR part of the ARMA model has order $p$.\n",
    "  - The MA part of the ARMA model has order $q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore generating some data as an ARIMA process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import six.moves\n",
    "def arima_process(size, ar_coefs, ma_coefs, d=0):\n",
    "    \"\"\"Simulate a series from an arima model.\"\"\"\n",
    "    arma = ArmaProcess(ar_coefs, ma_coefs)\n",
    "    arma_series = arma.generate_sample(size + d)\n",
    "    # Integrate d times.\n",
    "    for i in six.moves.range(d):\n",
    "        arma_series = np.cumsum(arma_series)\n",
    "    return pd.Series(arma_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_arima_process(ax, size, ar_coefs, ma_coefs, d=0):\n",
    "    series = arima_process(size, ar_coefs, ma_coefs, d)\n",
    "    ax.plot(series.index, series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ar_coefs = [[0.5], [-0.5]]\n",
    "ma_coefs = [[0.5], [-0.5]]\n",
    "ds = [0, 1]\n",
    "\n",
    "fig, axs = plt.subplots(8, figsize=(14, 15))\n",
    "\n",
    "for ax, (ar, ma, d) in six.moves.zip(axs, itertools.product(ar_coefs, ma_coefs, ds)):\n",
    "    plot_arima_process(ax, 250, ar, ma, d)\n",
    "    ax.set_title(\"ARIMA Process with AR={}, MA={}, d={}\".format(\n",
    "        str(list(ar)), str(list(ma)), d))\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, one final time, fitting an ARIMA model recovers the coefficients of a simulated ARIMA process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study:  Electrical Equipment Orders Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's attempt to fit an ARIMA model to a series of electrical orders, and use it to forecast future values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = os.path.join('.', 'data', 'elec-equip.csv')\n",
    "df = pd.read_csv(file_name)\n",
    "electric_series = pd.Series(df['x'].values, \n",
    "                            pd.DatetimeIndex(start='01-1996', end='12-2011', freq='M'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(14, 4))\n",
    "plot_series_and_difference(axs, electric_series, \"Electric Sales By Month\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The original series is clearly **not** stationary, so we differenced it.  Ther resulting series looks stationary, but let's use a test to check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electric_differences = electric_series.diff()[1:]\n",
    "test = sm.tsa.stattools.adfuller(electric_differences)\n",
    "print(\"ADF p-value for differenced electric series: {0:2.2f}\".format(test[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the differenced series is stationary, and we can attempt to model it as an ARMA process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to determine the correct order for the AR and MA models, so let's look at the auto and partial auto correlation plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 3))\n",
    "_ = sm.graphics.tsa.plot_acf(electric_differences, lags=25, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 3))\n",
    "_ = sm.graphics.tsa.plot_pacf(electric_differences, lags=25, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the following evidence:\n",
    "  - Oscillatory behaviour in the autocorrelation plot.\n",
    "  - Three significant partial autocorrelations at the beginning of the plot.\n",
    "  \n",
    "This suggests that the differenced series may be well modeled as an $AR(3)$ series.\n",
    "\n",
    "Since we applied *one* difference to achieve seasonality, this suggests that we should model the *original* series as an $ARIMA(3, 1, 0)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "electric_model = ARIMA(electric_series, order=(3, 1, 0)).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ARIMA(3, 1, 0) coefficients from Electric model:\\n  Intercept {0:2.2f}\\n  AR {1}\".format(\n",
    "    electric_model.params[0], \n",
    "        format_list_of_floats(list(electric_model.params[1:]))\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simulate some data using these coefficients and see if it looks like our differenced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, figsize=(14, 8))\n",
    "\n",
    "ax[0].plot(electric_differences.index, electric_differences)\n",
    "ax[0].set_title(\"First Differences of Electric Data\")\n",
    "\n",
    "for i in range(1, 4):\n",
    "    simulated_data = auto_regressive_process(len(electric_differences), \n",
    "                                             list(electric_model.params)[1:])\n",
    "    simulated_data.index = electric_differences.index\n",
    "    ax[i].plot(simulated_data.index, simulated_data)\n",
    "    ax[i].set_title(\"Simulated Data from Electric Model Fit\")\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use the fit model to project electric sales into the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electric_series.reindex(pd.DatetimeIndex(start='01-1996', end='12-2012', freq='M'))\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(14, 4))\n",
    "ax.plot(electric_series.index, electric_series)\n",
    "fig = electric_model.plot_predict('2011-11-30', '2013', \n",
    "                                  dynamic=True, ax=ax, plot_insample=False)\n",
    "\n",
    "_ = ax.legend().get_texts()[1].set_text(\"95% Prediction Interval\")\n",
    "_ = ax.legend(loc=\"lower left\")\n",
    "\n",
    "_ = ax.set_title(\"Electric Series Forcasts from ARIMA Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zooming in, we can see that we **have** done a bit better than the naive forecast of predicting the last observed value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "electric_series.reindex(pd.DatetimeIndex(start='01-1996', end='12-2012', freq='M'))\n",
    "\n",
    "fig, ax = plt.subplots(1, figsize=(14, 4))\n",
    "ax.plot(electric_series['2010':].index, electric_series['2010':])\n",
    "fig = electric_model.plot_predict('2011-11-30', '2013', \n",
    "                                  dynamic=True, ax=ax, plot_insample=False)\n",
    "\n",
    "_ = ax.legend().get_texts()[1].set_text(\"95% Prediction Interval\")\n",
    "_ = ax.legend(loc=\"lower left\")\n",
    "\n",
    "_ = ax.set_title(\"Electric Series Forecasts from ARIMA Model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If our model is good, the residuals should have no patterns whatsoever, i.e. be *white noise*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 3))\n",
    "ax.plot(electric_model.resid.index, electric_model.resid)\n",
    "ax.set_title(\"Residuals from Electric Model\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to diagnose this is to check if any autocorrelation remains in the residuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 3))\n",
    "_ = sm.graphics.tsa.plot_acf(electric_model.resid, lags=50, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the autocorrelations are within the confidence bands, so it looks like our model fits the data well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Selection: AIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We made our selection of model based on the autocorrelation and partial autocorrelation plot, which is a good first step.\n",
    "\n",
    "The overall procedure we followed is called the **Box-Jenkins Method**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyndmann recommends also trying a few models that are \"close by\", varying the ARMA parameters slightly, and then selecting the best model from the results using an information criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = [\n",
    "    {'AR': 3, 'MA': 0},\n",
    "    {'AR': 2, 'MA': 0},\n",
    "    {'AR': 4, 'MA': 0},\n",
    "    {'AR': 3, 'MA': 1}\n",
    "]\n",
    "\n",
    "models = {}\n",
    "for params in parameters:\n",
    "    models[(params['AR'], params['MA'])] = ARIMA(electric_series, order=(params['AR'], 1, params['MA'])).fit()\n",
    "    \n",
    "for model_params in models:\n",
    "    print(\"ARIMA({}, 1, {}) AIC: {}\".format(model_params[0], model_params[1], models[model_params].aic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $ARIMA(3, 1, 1)$ model has a *slightly* better AIC, so following Hyndmann, we would select this as our final model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall methodology for producing an ARIMA model is summarized in the following flowchart, taken from Hyndmann.\n",
    "\n",
    "![Hyndmann Modeling Process](img/hyndman-modeling-process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Case Study: Baseball Searches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's complete our day with one more case study, the baseball search series we have been referencing the entire day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, figsize=(14, 4))\n",
    "plot_series_and_difference(axs, google_trends['baseball'], 'baseball')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the differenced series **was** found to be stationary, but the partial autocorrelation plot had a curious feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(16, 4))\n",
    "\n",
    "_ = sm.graphics.tsa.plot_pacf(baseball_diff, lags=3*52, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That spike at $52$ weeks is something we don't know how to deal with yet!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a generalization of ARIMA that handles seasonality, which is included in the (development) version of `statsmodels` as SARIMAX.  The seasonal ARIMA model fits sub-ARIMA models to each seasonal sequence of data.\n",
    "\n",
    "A seasonal ARIMA model has more parameters\n",
    "\n",
    "$$ SARIMA(p, d, q)(sp, sd, sq)_k$$\n",
    "\n",
    "The $p$, $d$, and $q$ indices mean exactly the same thing as before.\n",
    "\n",
    "The $k$ is the length of a season, i.e., the observation window that exhibits periodic behaviour."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example we observe the following:\n",
    "  - The seasonal behaviour in the partial autocorrelation plot happens at $52$ weeks.\n",
    "  - There is only *one* significant seasonal partial autocorrelation.\n",
    "  - There is only *one* significant initial partial autocorrelation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Altogether, this means that an appropriate model would be a\n",
    "\n",
    "$$ SARIMA(1, 1, 0)(1, 0, 0)_{52} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baseball_series = google_trends['baseball']\n",
    "\n",
    "baseball_model = SARIMAX(baseball_series, order=(1, 1, 0), seasonal_order=(1, 0, 0, 52)).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the residuals for white noise'ness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 3))\n",
    "ax.plot(baseball_model.resid.index, baseball_model.resid)\n",
    "ax.set_title(\"Residuals from Baseball Model\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(14, 3))\n",
    "_ = sm.graphics.tsa.plot_acf(baseball_model.resid, lags=3*52, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one significant autocorrelation, but we expect at least a couple due to random chance, so this is satisfying."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To wrap up, let's see how the forecasts look."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas is a bit fussy about reindexing here, we need to extend the dates but\n",
    "# Pandas's weekly series does not align with google's.\n",
    "baseball_series.index = pd.DatetimeIndex(start='10-7-2011', end='9-25-2016', freq='W')\n",
    "baseball_series = baseball_series.reindex(pd.DatetimeIndex(\n",
    "        start='10-7-2011', end='2020', freq='W'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(16, 4))\n",
    "ax.plot(baseball_series.index, baseball_series)\n",
    "\n",
    "preds = baseball_model.predict('2016-09-17', '2020', \n",
    "                   dynamic=True, ax=ax, plot_insample=False)\n",
    "\n",
    "ax.plot(preds.index, preds)\n",
    "ax.set_title(\"Forecasts for Baseball Search Popularity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
