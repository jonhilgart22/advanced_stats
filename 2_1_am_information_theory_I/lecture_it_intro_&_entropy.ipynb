{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"https://thespectrumofriemannium.files.wordpress.com/2012/02/it.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "By The End Of This Session You Should Be Able To:\n",
    "----\n",
    "\n",
    "- Explain the general principles of Information Theory\n",
    "- Define Entropy in the context of Information Theory\n",
    "- Explain how Entropy can be used in statistics / machine learning\n",
    "- Calculate the entropy of common statistical events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Information Theory (IT)\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"http://gscim.com/Information_Theory/Shannon_communication.gif\" height=\"500\"/></center>\n",
    "<br>\n",
    "The goal of IT is to define the fundamental limits on signal processing and communication, such as data compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Information Theory Greatest Hits\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"images/mobile.png\" height=\"500\"/></center>\n",
    "\n",
    "- Modern computers\n",
    "- Internet\n",
    "- Mobile phone communication / Telecommunications systems\n",
    "- Computational linguists modeling\n",
    "- The invention of the compact disc (cd)\n",
    "- Understanding of black holes\n",
    "- Voyager missions to deep space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "IT History\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"http://stanstudio.com/wp-content/uploads/2012/02/Claude_Shannon_Juggling.jpg\" height=\"500\"/></center>\n",
    "\n",
    "Originally proposed by Claude E. Shannon in __1948__ in a landmark paper entitled \"A Mathematical Theory of Communication\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\"Entropy\" is the key concept in information theory\n",
    "-----\n",
    "\n",
    "<center><img src=\"http://imagecache5d.allposters.com/watermarker/62-6264-ZSG5100Z.jpg?ch=671&cw=894&type=cns\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy in Physics\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://d2jmvrsizmvf4x.cloudfront.net/rvkaVrvITeYKGO3EMmeG_entropy.jpg\" height=\"500\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy in Thermodynamics\n",
    "-----\n",
    "\n",
    "The second law of thermodynamics states that the total entropy of an isolated system can only increase over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<center><img src=\"http://s2.quickmeme.com/img/cd/cd9ac5d71167288007fe7d9b45dc8faa7229529028a54797cba49b55e0700963.jpg\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy in Information Theory\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://s-media-cache-ak0.pinimg.com/236x/f7/c8/97/f7c897fdc5d25be38359705a15336063.jpg\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Claude Shannon defined the fundamental units of __information__, the smallest possible chunk that cannot be divided any further.\n",
    "\n",
    "He called the units \"bits\". Bit is short for binary digit: 0 or 1.\n",
    "\n",
    "Groups of which can be used to encode __any__ message. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/def.png\" height=\"500\"/></center>\n",
    "\n",
    "Shannon entropy is the quantity H, a measure of the information in a message."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Sum up the probabilities of the all possible symbols (x) that might turn up in a message, weighted by the number of bits needed to represent the value of that symbol (x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Logarithms review\n",
    "-----\n",
    "\n",
    "<center><img src=\"https://people.richland.edu/james/lecture/m116/logs/log2.gif\" height=\"500\"/></center>\n",
    "\n",
    "A logarithm is an inverse exponential function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "b<sup>x</sup> = y is equivalent to saying x = log<sub>b</sub>y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Properties of exponentials and logarithms\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"http://science.larouchepac.com/gauss/ceres/InterimII/Arithmetic/Primes/Log_Exp_inverts.jpg\" width=\"400\"/></center>\n",
    "\n",
    "Exponential functions grow at a distressingly fast rate, as anyone who has ever tried to pay off a credit card balance understands. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Thus, logarithms, inverse exponential functions, grow very slowly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Logarithms arise in any process where things are repeatedly halved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "What is binary search? Why is it fast? What is the maximum of binary searches will it find an item in a million?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given a sorted array, we start in the middle n/2 and look for our item. If we don't find it we take either the upper or lower half and repeat until we find it.\n",
    "\n",
    "Binary search is a good example of an O(log<sub>2</sub>n) algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "If we have 1 million items (haystack), what is the most number of binary searches needed to find a specific item (needle)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.931568569324174\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "from math import ceil, log\n",
    "\n",
    "print(log(1_000_000, 2))\n",
    "print(ceil(log(1_000_000, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bits\n",
    "--------\n",
    "\n",
    "If we have legnth of 1, there are 2 bits: {0, 1}\n",
    "\n",
    "If we have length of 2, there are 4 bits: {00, 01, 10, 11}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bits & Logarithms\n",
    "--------\n",
    "\n",
    "How many bits do we need to represent any one of n different possibilities (one of n items or the integers from 1 to n)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The key observation is that there must be at least __n__ different bit patterns of length __w__. \n",
    "\n",
    "Since the number of different bit patterns doubles as you add each bit, we need at least __w__ bits where __2<sup>w</sup> = n__.\n",
    "\n",
    "We need w = log<sub>2</sub>n bits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of items|# of bits\n",
      "         2         1\n",
      "         6         3\n",
      "        52         6\n",
      "     1,000        10\n",
      "    10,000        14\n",
      "   100,000        17\n",
      " 1,000,000        20\n"
     ]
    }
   ],
   "source": [
    "Ns = [2, 6, 52, 1_000, 10_000, 100_000, 1_000_000]\n",
    "\n",
    "print(f\"{'# of items':>10}|{'# of bits':>9}\")\n",
    "for n in Ns:\n",
    "    print(f\"{n:>10,} {ceil(log(n, 2)):>9}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy Formula\n",
    "-------\n",
    "\n",
    "H = -Œ£ p<sub>i</sub> ‚Ä¢ log<sub>2</sub>p<sub>i</sub>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "For each symbol in a message, find the probability then multiple it by the log of it. \n",
    "\n",
    "Sum up all of those and take the negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The base-2 measure of entropy has sometimes been called the \"Shannon Entropy\" in Claude Shannon's honor. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Base-2 logarithms \n",
    "------\n",
    "\n",
    "Base-2 logarithms create units called __bits__ (or shannons)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Natural logarithm, with base e, then our units would be __nats__. \n",
    "\n",
    "One nat is the amount of information gained by observing an event of probability 1/e."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy Symbol\n",
    "-------\n",
    "<br>\n",
    "<center><img src=\"images/e_f.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "‚Ñç is very ugly Unicode so I'll just use H."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We are only to use base-2 and discrete items. \n",
    "\n",
    "IT extends to other bases and continuous levels of measurement. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy in Statistics\n",
    "-----\n",
    "<br>\n",
    "<center><img src=\"https://imgs.xkcd.com/comics/im_so_random.png\" height=\"400\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy is the measure of the uncertainty in a random variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Intuitively, the entropy of a discrete random variable X is a measure of the amount of uncertainty associated with the value of X when only its distribution is known."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The entropy of variable X is defined as:  \n",
    "H(X) = -Œ£ p(x) ‚Ä¢ log<sub>2</sub>p(x)\n",
    "\n",
    "p(x) = Pr{X = x}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy of a single coin flip\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"http://68.media.tumblr.com/bb49b60c0e17387aef2d3da24f0ef40a/tumblr_n30n5iMJgT1sa11jco1_500.gif\" height=\"500\"/></center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "From the probability lecture, what is the statisical term we use to model a single toss of a coin?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Bernoulli trial__: There is a single outcome of two possible for each trial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "Can a Bernoulli process, a series of Bernoulli trials, model unfair coins?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Yes. We can model the probabilities of coming up heads (not heads /tails) as a number between 0 and 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's say we have a fair coin, the probability occurring p(heads) and p(tails) are each 50%.\n",
    "\n",
    "H = -Œ£(p<sub>i</sub> ‚Ä¢ log<sub>2</sub>(p<sub>i</sub>))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "H = -(.5*log(.5, 2) + .5*log(.5, 2))\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Once we have flipped a coin, we have gained one bit of information or reduced our uncertainty by one bit.\n",
    "\n",
    "This makes intuitive sense: 0 = Heads, 1 = Tails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy of a single \"weighted\" coin flip\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"https://izbicki.me/img/uploads/2011/11/coins-all.jpg\" height=\"500\"/></center>\n",
    "\n",
    "p(H) = .2  \n",
    "P(¬¨H) = 1-.2 = .8\n",
    "\n",
    "What is H? \n",
    "\n",
    "H = -Œ£(pi ‚Ä¢ log2(pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.721928094887\n"
     ]
    }
   ],
   "source": [
    "from numpy import log2\n",
    "\n",
    "H = -(.2*log2(.2) + .8*log2(.8))\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "---- \n",
    "\n",
    "Is there more or less entropy in a the biased coin?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Less entropy__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'log2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-636df1b13af7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mp_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_heads\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_heads\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mp_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mlog2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mp_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mH\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Way less entropy, aka far more predicatable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'log2' is not defined"
     ]
    }
   ],
   "source": [
    "p_heads = .001\n",
    "H = -(p_heads*log2(p_heads) + (1-p_heads)*log2(1-p_heads))\n",
    "print(H) # Way less entropy, aka far more predicatable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/brianspiering/anaconda3/envs/stats/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in log2\n",
      "  \"\"\"\n",
      "/Users/brianspiering/anaconda3/envs/stats/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "from numpy import log2 \n",
    "\n",
    "# What will this be?\n",
    "p_heads = 0\n",
    "H = -(p_heads*log2(p_heads) + (1-p_heads)*log2(1-p_heads))\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The entropy is zero: each toss of the coin delivers no new information as the outcome of each coin toss is always certain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Binary entropy function\n",
    "------\n",
    "\n",
    "The special case of information entropy for a random variable with two outcomes:\n",
    "<br><br>\n",
    "<center><img src=\"images/bin_ent.svg\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "H = -p_success*log2(p_success) - (1-p_success)*log2(1-p_success)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Bernoulli random variable entropy as a function of probability of success.\n",
    "------\n",
    "\n",
    "<center><img src=\"images/curve.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/curve.png\" width=\"500\"/></center>\n",
    "\n",
    "What is the maximum entropy value? At which probability of success will that occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The maximum entropy is H=1 and will happen when P(Success) = .5, aka discrete uniform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "Which has less entropy a flip of fair coin or the roll of a fair dice? Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Identifying the outcome of a fair coin flip (2 equally likely outcomes) provides less information (lower entropy) than specifying the outcome from a roll of a die (6 equally likely outcomes)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src=\"images/events.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "What is the entropy of a 2 (fair) coin flips?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Since they are independent,   \n",
    "H(Coin 1) + H(Coin 2) = H(2 Coins)\n",
    "1 + 1 = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy of a 2 (fair) coin flips\n",
    "------\n",
    "<br>\n",
    "<center><img src=\"images/coins.png\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "H = -Œ£(p<sub>i</sub> ‚Ä¢ log<sub>2</sub>(p<sub>i</sub>))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n"
     ]
    }
   ],
   "source": [
    "H = -(.5*log2(.5) + \n",
    "      .5*log2(.5) +\n",
    "      .5*log2(.5) +\n",
    "     . 5*log2(.5))\n",
    "print(H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What is the Shannon entropy of the English alphabet*?\n",
    "-----\n",
    "\n",
    "English has 26 letters (a-z). \n",
    "\n",
    "<sub>* if each character is equally likely</sub>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7004\n"
     ]
    }
   ],
   "source": [
    "n = 26\n",
    "H = -((1/n)*log2(1/n)*n)\n",
    "print(f\"{H:.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.7004\n"
     ]
    }
   ],
   "source": [
    "print(f\"{log2(n):.5}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "https://www.quora.com/What-is-the-27th-letter-of-the-English-alphabet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "What is the Shannon entropy of \"hello\" in ASCII?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "H = -Œ£(p<sub>i</sub> ‚Ä¢ log<sub>2</sub>(p<sub>i</sub>))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "p(\"h\") = 1/5   \n",
    "p(\"e\") = 1/5   \n",
    "p(\"l\") = 1/5  \n",
    "p(\"l\") = 1/5    \n",
    "p(\"0\") =  1/5  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "p(\"h\") = p(\"e\") =  p(\"0\") = 1/5   \n",
    "p(\"l\") = 2/5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.92\n"
     ]
    }
   ],
   "source": [
    "H = -((1/5)*log2(1/5) + \n",
    "      (1/5)*log2(1/5) +\n",
    "      (1/5)*log2(1/5) +\n",
    "      (2/5)*log2(2/5))\n",
    "print(f\"{H:.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Shannon entropy\n",
    "-----\n",
    "\n",
    "The average minimum number of bits needed to encode a symbol from a string of symbols, based on the probability of each symbol appearing in that string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "If H = 1.92, how many bits do you need per symbol to encode \"hello\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "2 bits.\n",
    "\n",
    "The entropy is 1.92 but bits are discrete / integers so we take the ceiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In other words, Shannon Entropy is the‚Ä¶\n",
    "------\n",
    "\n",
    "<center><img src=\"http://www.science4all.org/wp-content/uploads/2013/03/Noisy-Communication2.png\" width=\"500\"/></center>\n",
    "\n",
    "Absolute limit on the best possible lossless encoding or compression of any communication assuming that the communication may be represented as a sequence of iid random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The entropy of a source that emits a sequence of N symbols that are independent and identically distributed (iid) is N¬∑H bits (per message of N symbols)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hoffman Encoding Redux\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Allocate short codewords to highly probable bit strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Reserve longer codewords to less probable bit strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "[Huffman encoding animation](https://people.ok.ubc.ca/ylucet/DS/Huffman.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-------\n",
    "\n",
    "True or False:\n",
    "\n",
    "Entropy is always non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__True__: Entropy is always non-negative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy in other words\n",
    "------\n",
    "\n",
    "Entropy is the minimum descriptive complexity of a random variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"images/self.svg\" width=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Here, I(x) is the self-information, which is the entropy contribution of an individual message, and ùîºX is the expected value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A property of entropy is that it is maximized when all the messages in the message space are equiprobable p(x) = 1/n; i.e., most unpredictable, in which case H(X) = log n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-------\n",
    "\n",
    "When does a system have zero entropy? What would call in statistics?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There is a single state, aka not a Random Variable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy in yet other words\n",
    "------\n",
    "\n",
    "Entropy is the lower bound on the average number of yes/no questions to guess the state of a variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Remember: Binary search (yes/no) to guess where the needle is in a sorted haystack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy (one last time)\n",
    "-----\n",
    "\n",
    "Entropy is sometimes called a measure of surprise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "------\n",
    "\n",
    "Which discrete distribution has the maximum entropy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "__Uniform distribution__\n",
    "\n",
    "The frequencies histogram is even. Knowing the distribution gives us no prior information.\n",
    "\n",
    "It is the most suprising distribution to sample from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Check for understanding\n",
    "-----\n",
    "\n",
    "Which discrete distribution has the minimum entropy (which is zero)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/4/48/Dirac_distribution_PDF.svg/325px-Dirac_distribution_PDF.svg.png\" height=\"500\"/></center>\n",
    "\n",
    "__Dirac delta function__, aka an impluse function.\n",
    "\n",
    "All its probability mass is in a single state. The distribution has no uncertainty, zero entropy.\n",
    "\n",
    "Knowing the distribution gives us all the prior information we could use.\n",
    "\n",
    "It is the least suprising distribution to sample from."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Summary\n",
    "----\n",
    "\n",
    "- Information theory is concerned with representing data in a compact fashion.\n",
    "- Entropy is the measure of the uncertainty in a random variable.\n",
    "- The entropy of a discrete random variable X is a measure of the amount of uncertainty associated with the value of X when only its distribution is known.\n",
    "- Entropy is calculated as: H = -Œ£(p<sub>i</sub> ‚Ä¢ log<sub>2</sub>(p<sub>i</sub>))\n",
    "- That formula allows us to common across domains and random variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "Bonus Material\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let X be a discrete random variable with alphabet ùìß and probability mass function p(x)\n",
    "\n",
    "p(x) = Pr{X = x}, x ‚àà ùìß"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The alphabet ùìß is all possible outcomes (I know that is yet another \"x\" but we need it)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Entropy is the expected ‚Äúcompressibility‚Äù of a single bit under the best encoding. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "[Using IT to calculate how many tweets could there be?](https://what-if.xkcd.com/34/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Differential entropy\n",
    "-----\n",
    "\n",
    "Differential entropy (also referred to as continuous entropy) is a concept in information theory that began as an attempt by Shannon to extend the idea of (Shannon) entropy, a measure of average surprisal of a random variable, to continuous probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<br>\n",
    "<br>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
